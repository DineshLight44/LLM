{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25885653",
      "metadata": {
        "id": "25885653"
      },
      "source": [
        "# Fine-tuning an LLM with LoRA (Low-Rank Adaptation)\n",
        "\n",
        "This notebook demonstrates how to fine-tune a pre-trained Large Language Model (LLM) on a specific dataset using **LoRA (Low-Rank Adaptation)**, a popular Parameter-Efficient Fine-Tuning (PEFT) technique.\n",
        "\n",
        "## Objectives\n",
        "1. Load a pre-trained LLM (Open Source).\n",
        "2. Load a specialized dataset.\n",
        "3. Apply LoRA to the model to decrease the number of trainable parameters.\n",
        "4. Fine-tune the model using the Hugging Face `Trainer`.\n",
        "5. Run inference to test the fine-tuned model.\n",
        "\n",
        "## What is LoRA?\n",
        "LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. This greatly reduces the number of trainable parameters for downstream tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91bc88ef",
      "metadata": {
        "id": "91bc88ef"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "# We need transformers for the model, peft for LoRA, datasets for loading data, and accelerate for optimization.\n",
        "%pip install -q transformers peft datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9d8dc1",
      "metadata": {
        "id": "ca9d8dc1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "\n",
        "# Set device to CUDA if available, otherwise CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7019c8ce",
      "metadata": {
        "id": "7019c8ce"
      },
      "source": [
        "## 1. Load Pre-trained Model and Tokenizer\n",
        "\n",
        "We will use `facebook/opt-350m` (or a similar size GPT-like model) as our base model. It is small enough to run on most consumer hardware for demonstration purposes but acts like a standard causal LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862c5767",
      "metadata": {
        "id": "862c5767"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/opt-350m\"\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load Model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Base model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b40a981",
      "metadata": {
        "id": "4b40a981"
      },
      "source": [
        "## 2. Load and Prepare Dataset\n",
        "\n",
        "We will use the `Abirate/english_quotes` dataset. It contains quotes and tags. We will fine-tune the model to familiarize it with this style of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3498e9b",
      "metadata": {
        "id": "a3498e9b"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"Abirate/english_quotes\")\n",
        "\n",
        "# Inspect a sample\n",
        "print(\"Sample data:\", dataset['train'][0])\n",
        "\n",
        "# Preprocessing function to tokenize the data\n",
        "# We iterate over the dataset and tokenize the 'quote' column\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['quote'], padding=\"max_length\", truncation=True, max_length=64)\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Select a subset for faster training demonstration (optional)\n",
        "tokenized_datasets['train'] = tokenized_datasets['train'].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# We removed columns that are not needed for training to keep it clean\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"quote\", \"author\", \"tags\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36895819",
      "metadata": {
        "id": "36895819"
      },
      "source": [
        "## 3. Apply PEFT (LoRA)\n",
        "\n",
        "Here we define the LoRA configuration.\n",
        "- `r`: Rank of the update matrices. Lower means fewer parameters.\n",
        "- `lora_alpha`: Scaling factor for the weights.\n",
        "- `lora_dropout`: Dropout probability for LoRA layers.\n",
        "- `bias`: Bias setting ('none', 'all', or 'lora_only').\n",
        "- `task_type`: Task type (CAUSAL_LM for text generation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678668ec",
      "metadata": {
        "id": "678668ec"
      },
      "outputs": [],
      "source": [
        "# Define LoRA Config\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters to see the efficiency\n",
        "# You will see that we are only training a very small percentage of the total params\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f90300d",
      "metadata": {
        "id": "2f90300d"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "We use the Hugging Face `Trainer` to fine-tune the model. This abstracts the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49aef25c",
      "metadata": {
        "id": "49aef25c"
      },
      "outputs": [],
      "source": [
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=1,  # Kept low for demonstration purposes. Increase for better results.\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\", # Save model at the end of epoch\n",
        "    report_to=\"none\"  # Disable wandb/mlflow for this demo\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95810388",
      "metadata": {
        "id": "95810388"
      },
      "source": [
        "## 5. Save the Fine-Tuned Model\n",
        "\n",
        "We save the adapter weights separately from the base model. This is the key advantage of PEFT; we only share the small adapter file (~MBs) instead of the full model (~GBs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9031dacd",
      "metadata": {
        "id": "9031dacd"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"./lora_fine_tuned_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "print(f\"LoRA adapters saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a980af30",
      "metadata": {
        "id": "a980af30"
      },
      "source": [
        "## 6. Inference\n",
        "\n",
        "Let's test the model. We need to load the base model and then attach the trained LoRA adapters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Configuration\n",
        "# --------------------------------------------------\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load tokenizer (must match base model)\n",
        "# --------------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load base model (clean state)\n",
        "# --------------------------------------------------\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "base_model.to(device)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load LoRA adapters\n",
        "# --------------------------------------------------\n",
        "inference_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    model_save_path\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# IMPORTANT: Set evaluation mode\n",
        "# --------------------------------------------------\n",
        "inference_model.eval()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# (Optional but recommended) Merge LoRA weights\n",
        "# --------------------------------------------------\n",
        "# This removes adapter overhead and stabilizes output\n",
        "merged_model = inference_model.merge_and_unload()\n",
        "merged_model.eval()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Text generation function\n",
        "# --------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def generate_text(prompt, model, max_new_tokens=12):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=False\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,        # deterministic decoding for evaluation\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Test inference\n",
        "# --------------------------------------------------\n",
        "test_prompt = \"Complete the following sentence in a neutral academic tone:\\nLife is\"\n",
        "\n",
        "print(\"---- Generated Text (Fine-tuned Model) ----\")\n",
        "print(generate_text(test_prompt, merged_model))\n"
      ],
      "metadata": {
        "id": "0qZMYz-DrGpp"
      },
      "id": "0qZMYz-DrGpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcmjbTvSrs0V"
      },
      "id": "PcmjbTvSrs0V",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}