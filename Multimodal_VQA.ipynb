{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef50f04",
   "metadata": {},
   "source": [
    "# Multimodal LLM for Image and Text-Based Question Answering\n",
    "\n",
    "This notebook implements a lightweight multimodal LLM for visual question answering (VQA) that can run on 4GB VRAM.\n",
    "\n",
    "**Model Used:** BLIP-2 (Salesforce) - Optimized for low VRAM usage\n",
    "\n",
    "**Features:**\n",
    "- Image understanding and analysis\n",
    "- Visual question answering\n",
    "- Image captioning\n",
    "- Multi-turn conversations about images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59176f89",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a07b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers accelerate pillow torch torchvision bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bfc23",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223352f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db401c",
   "metadata": {},
   "source": [
    "## 3. Load Lightweight Multimodal Model\n",
    "\n",
    "We'll use BLIP (Bootstrapping Language-Image Pre-training) which is efficient for 4GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65767f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load BLIP model for Visual Question Answering\n",
    "print(\"Loading BLIP VQA model...\")\n",
    "vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "\n",
    "# Load BLIP model for Image Captioning\n",
    "print(\"Loading BLIP Captioning model...\")\n",
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddb1ed",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eab7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_source):\n",
    "    \"\"\"\n",
    "    Load image from URL or local path\n",
    "    \n",
    "    Args:\n",
    "        image_source: URL string or local file path\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image object\n",
    "    \"\"\"\n",
    "    if image_source.startswith('http://') or image_source.startswith('https://'):\n",
    "        response = requests.get(image_source)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_source).convert('RGB')\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_caption(image, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate caption for an image\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or image path/URL\n",
    "        max_length: Maximum caption length\n",
    "    \n",
    "    Returns:\n",
    "        Generated caption string\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "    \n",
    "    inputs = caption_processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = caption_model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "def answer_question(image, question, max_length=50):\n",
    "    \"\"\"\n",
    "    Answer a question about an image\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or image path/URL\n",
    "        question: Question string\n",
    "        max_length: Maximum answer length\n",
    "    \n",
    "    Returns:\n",
    "        Answer string\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "    \n",
    "    inputs = vqa_processor(image, question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = vqa_model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    answer = vqa_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def analyze_image(image, questions=None, generate_cap=True):\n",
    "    \"\"\"\n",
    "    Comprehensive image analysis with caption and Q&A\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or image path/URL\n",
    "        questions: List of questions to ask about the image\n",
    "        generate_cap: Whether to generate caption\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with caption and answers\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if generate_cap:\n",
    "        results['caption'] = generate_caption(image)\n",
    "    \n",
    "    if questions:\n",
    "        results['qa'] = []\n",
    "        for q in questions:\n",
    "            answer = answer_question(image, q)\n",
    "            results['qa'].append({'question': q, 'answer': answer})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a96a5e",
   "metadata": {},
   "source": [
    "## 5. Example 1: Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image URL (a dog on the beach)\n",
    "image_url = \"https://images.unsplash.com/photo-1544568100-847a948585b9?w=800\"\n",
    "\n",
    "# Load and display image\n",
    "image = load_image(image_url)\n",
    "display(image.resize((400, 300)))\n",
    "\n",
    "# Generate caption\n",
    "caption = generate_caption(image)\n",
    "print(f\"\\n Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb550c",
   "metadata": {},
   "source": [
    "## 6. Example 2: Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c94f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same image\n",
    "display(image.resize((400, 300)))\n",
    "\n",
    "# Ask questions about the image\n",
    "questions = [\n",
    "    \"What animal is in the image?\",\n",
    "    \"What is the color of the dog?\",\n",
    "    \"Where is the dog?\",\n",
    "    \"What is the dog doing?\",\n",
    "]\n",
    "\n",
    "print(\"\\n Visual Question Answering:\\n\")\n",
    "for question in questions:\n",
    "    answer = answer_question(image, question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17574845",
   "metadata": {},
   "source": [
    "## 7. Example 3: Multi-Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90300a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple images\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=800\",  # Mountain landscape\n",
    "    \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?w=800\",  # Cat\n",
    "]\n",
    "\n",
    "for idx, url in enumerate(image_urls, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Image {idx}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    img = load_image(url)\n",
    "    display(img.resize((400, 300)))\n",
    "    \n",
    "    # Generate caption\n",
    "    cap = generate_caption(img)\n",
    "    print(f\"\\nüìù Caption: {cap}\")\n",
    "    \n",
    "    # Ask generic questions\n",
    "    print(\"\\nüîç Q&A:\")\n",
    "    generic_questions = [\n",
    "        \"What is in this image?\",\n",
    "        \"What colors are dominant?\",\n",
    "    ]\n",
    "    \n",
    "    for q in generic_questions:\n",
    "        a = answer_question(img, q)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42d05c",
   "metadata": {},
   "source": [
    "## 8. Example 4: Interactive Q&A Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image for interactive Q&A\n",
    "test_image_url = \"https://images.unsplash.com/photo-1551963831-b3b1ca40c98e?w=800\"  # Breakfast\n",
    "test_image = load_image(test_image_url)\n",
    "\n",
    "print(\"Image for analysis:\")\n",
    "display(test_image.resize((400, 300)))\n",
    "\n",
    "# First, get a caption to understand what's in the image\n",
    "print(\"\\n Image Caption:\")\n",
    "print(generate_caption(test_image))\n",
    "\n",
    "# Interactive Q&A\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Interactive Q&A Session\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou can ask questions about this image.\")\n",
    "print(\"Type 'quit' to exit.\\n\")\n",
    "\n",
    "# For notebook, we'll demonstrate with predefined questions\n",
    "# In a real interactive scenario, you would use input()\n",
    "demo_questions = [\n",
    "    \"What food items are visible?\",\n",
    "    \"Is this breakfast or dinner?\",\n",
    "    \"What is the main dish?\",\n",
    "    \"Are there any fruits?\",\n",
    "]\n",
    "\n",
    "for q in demo_questions:\n",
    "    print(f\"\\nüë§ User: {q}\")\n",
    "    answer = answer_question(test_image, q)\n",
    "    print(f\"ü§ñ Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf704d3",
   "metadata": {},
   "source": [
    "## 9. Example 5: Custom Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can upload your own image or use a local file path\n",
    "# For demonstration, we'll use another URL\n",
    "\n",
    "custom_image_url = \"https://images.unsplash.com/photo-1485965120184-e220f721d03e?w=800\"  # Bicycle\n",
    "\n",
    "# Comprehensive analysis\n",
    "custom_questions = [\n",
    "    \"What is the main object in this image?\",\n",
    "    \"What color is it?\",\n",
    "    \"Is this indoors or outdoors?\",\n",
    "    \"What is the weather like?\",\n",
    "    \"Are there any people?\",\n",
    "]\n",
    "\n",
    "results = analyze_image(\n",
    "    custom_image_url,\n",
    "    questions=custom_questions,\n",
    "    generate_cap=True\n",
    ")\n",
    "\n",
    "# Display image\n",
    "img = load_image(custom_image_url)\n",
    "display(img.resize((400, 300)))\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comprehensive Image Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Caption: {results['caption']}\\n\")\n",
    "\n",
    "print(\" Question & Answer:\")\n",
    "for qa in results['qa']:\n",
    "    print(f\"\\nQ: {qa['question']}\")\n",
    "    print(f\"A: {qa['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03be90",
   "metadata": {},
   "source": [
    "## 10. Memory Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b42a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n GPU Memory Statistics:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU - No GPU memory to monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec26cec",
   "metadata": {},
   "source": [
    "## 11. Advanced: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4048b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze_images(image_sources, question):\n",
    "    \"\"\"\n",
    "    Analyze multiple images with the same question\n",
    "    \n",
    "    Args:\n",
    "        image_sources: List of image URLs or paths\n",
    "        question: Question to ask about each image\n",
    "    \n",
    "    Returns:\n",
    "        List of results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, source in enumerate(image_sources, 1):\n",
    "        try:\n",
    "            image = load_image(source)\n",
    "            caption = generate_caption(image)\n",
    "            answer = answer_question(image, question)\n",
    "            \n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'source': source,\n",
    "                'caption': caption,\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'source': source,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example batch processing\n",
    "batch_images = [\n",
    "    \"https://images.unsplash.com/photo-1568605114967-8130f3a36994?w=800\",  # House\n",
    "    \"https://images.unsplash.com/photo-1469474968028-56623f02e42e?w=800\",  # Nature\n",
    "]\n",
    "\n",
    "batch_results = batch_analyze_images(batch_images, \"What is the main subject?\")\n",
    "\n",
    "print(\"\\n Batch Processing Results:\\n\")\n",
    "for result in batch_results:\n",
    "    if 'error' not in result:\n",
    "        print(f\"Image {result['index']}:\")\n",
    "        print(f\"  Caption: {result['caption']}\")\n",
    "        print(f\"  Q: {result['question']}\")\n",
    "        print(f\"  A: {result['answer']}\\n\")\n",
    "    else:\n",
    "        print(f\"Image {result['index']}: Error - {result['error']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfdfad",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook execution complete!\")\n",
    "print(\"\\nKey Features Implemented:\")\n",
    "print(\"- Image captioning\")\n",
    "print(\"- Visual question answering\")\n",
    "print(\"- Multi-image analysis\")\n",
    "print(\"- Batch processing\")\n",
    "print(\"- Memory-efficient design for 4GB VRAM\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
